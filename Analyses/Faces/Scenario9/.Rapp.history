save(MeanRunMet, file=paste(wd,'/MeanRunMet',sep=''))
save(SD.FIX, file=paste(wd,'/SD.FIX',sep=''))
save(SD.OLS, file=paste(wd,'/SD.OLS',sep=''))
save(SD.MIX, file=paste(wd,'/SD.MIX',sep=''))
save(MeanRunMetA, file=paste(wd,'/SD.FIXA',sep=''))
save(SD.FIXA, file=paste(wd,'/SD.FIXA',sep=''))
save(SD.OLSA, file=paste(wd,'/SD.FIXA',sep=''))
save(SD.MIXA, file=paste(wd,'/SD.FIXA',sep=''))
load(paste(wd,'/MeanRunMet',sep=''))
## Make a plot of the mean (with SD as err bars)#
xAxes <- c(1:5)#
colours <- c('#1f78b4','#33a02c','#b15928')#
colours <- c('#1b9e77','#d95f02','#7570b3')#
#
# Put mean FPR of ALE pID and pN at NA#
MeanRunMetA[['FPR']][,c('ALEpID','ALEpN')] <- NA
# Make separate plots#
metaLabels <- c('Fixed Effects MA','Random Effects MA','ALE pID', 'ALE pN', 'ALE Uncorrected')#
par(mfrow=c(1,1))#
# Overlap#
plot(MeanRunMetA[['Overlap']][1,]~xAxes,type='b',pch=4,lwd=3,lty=1,ylim=c(0,.6),ylab='Overlap',col=colours[1],cex.lab=1.4,cex.main=1.7,main=paste('Average Overlap',sep=''), xlab='Meta-analysis method',axes=FALSE)#
errbar(x=xAxes,y=MeanRunMetA[['Overlap']][1,],#
    yplus=MeanRunMetA[['Overlap']][1,]+SD.FIX[,'Overlap'],#
    yminus=MeanRunMetA[['Overlap']][1,]-SD.FIX[,'Overlap'],#
    add=TRUE,errbar.col=colours[1])#
lines(MeanRunMetA[['Overlap']][2,]~xAxes,type='b',pch=4,lwd=3,lty=3,col=colours[2])#
errbar(x=xAxes,y=MeanRunMetA[['Overlap']][2,],#
    yplus=MeanRunMetA[['Overlap']][2,]+SD.OLS[,'Overlap'],#
    yminus=MeanRunMetA[['Overlap']][2,]-SD.OLS[,'Overlap'],#
    add=TRUE,errbar.col=colours[2])#
lines(MeanRunMetA[['Overlap']][3,]~c(xAxes+.02),type='b',pch=4,lwd=3,lty=5,col=colours[3])  #
errbar(x=c(xAxes+.02),y=MeanRunMetA[['Overlap']][3,],#
    yplus=MeanRunMetA[['Overlap']][3,]+SD.MIX[,'Overlap'],#
    yminus=MeanRunMetA[['Overlap']][3,]-SD.MIX[,'Overlap'],#
    add=TRUE,errbar.col=colours[3])#
#
    legend('topright',legend=c('Fixed Effect','OLS', 'Mixed Effect'),col=colours,#
        lwd=3,lty=c(1,3,5),bty='n',title='Pooling subjects:', cex=1.4)#
    axis(1,at=xAxes,labels=metaLabels,cex.axis=1.2)#
    axis(2,at=seq(0,.6,.1),cex.axis=1.2)
# Power#
plot(MeanRunMetA[['Power']][1,]~xAxes,type='b',pch=4,lwd=3,lty=1,ylim=c(0,1),ylab='Power',col=colours[1],cex.lab=1.4,cex.main=1.7,main=paste('Average Power',sep=''), xlab='Meta-analysis method',axes=FALSE)#
errbar(x=xAxes,y=MeanRunMetA[['Power']][1,],#
    yplus=MeanRunMetA[['Power']][1,]+SD.FIX[,'Power'],#
    yminus=MeanRunMetA[['Power']][1,]-SD.FIX[,'Power'],#
    add=TRUE,errbar.col=colours[1])#
lines(MeanRunMetA[['Power']][2,]~xAxes,type='b',pch=4,lwd=3,lty=3,col=colours[2])#
errbar(x=xAxes,y=MeanRunMetA[['Power']][2,],#
    yplus=MeanRunMetA[['Power']][2,]+SD.OLS[,'Power'],#
    yminus=MeanRunMetA[['Power']][2,]-SD.OLS[,'Power'],#
    add=TRUE,errbar.col=colours[2])#
lines(MeanRunMetA[['Power']][3,]~c(xAxes+.02),type='b',pch=4,lwd=3,lty=5,col=colours[3])    #
errbar(x=c(xAxes+.02),y=MeanRunMetA[['Power']][3,],#
    yplus=MeanRunMetA[['Power']][3,]+SD.MIX[,'Power'],#
    yminus=MeanRunMetA[['Power']][3,]-SD.MIX[,'Power'],#
    add=TRUE,errbar.col=colours[3])#
    legend('topright',legend=c('Fixed Effect','OLS', 'Mixed Effect'),col=colours,#
        lwd=3,lty=c(1,3,5),bty='n',title='Pooling subjects:', cex=1.4)#
    axis(1,at=xAxes,labels=metaLabels,cex.axis=1.2)#
    axis(2,at=seq(0,1,.20),cex.axis=1.2)
# FPR#
plot(MeanRunMetA[['FPR']][1,c(1,2,5)]~xAxes[c(1,2,3)],type='b',pch=4,lwd=3,lty=1,ylim=c(0,.2),ylab='FPR',col=colours[1],cex.lab=1.4,cex.main=1.7,main=paste('Average FPR',sep=''), xlab='Meta-analysis method',axes=FALSE)#
errbar(x=xAxes[c(1,2,3)],y=MeanRunMetA[['FPR']][1,c(1,2,5)],#
    yplus=MeanRunMetA[['FPR']][1,c(1,2,5)]+SD.FIX[c(1,2,5),'FPR'],#
    yminus=MeanRunMetA[['FPR']][1,c(1,2,5)]-SD.FIX[c(1,2,5),'FPR'],#
    add=TRUE,errbar.col=colours[1])#
lines(MeanRunMetA[['FPR']][2,c(1,2,5)]~xAxes[c(1,2,3)],type='b',pch=4,lwd=3,lty=3,col=colours[2])#
errbar(x=xAxes[c(1,2,3)],y=MeanRunMetA[['FPR']][2,c(1,2,5)],#
    yplus=MeanRunMetA[['FPR']][2,c(1,2,5)]+SD.OLS[c(1,2,5),'FPR'],#
    yminus=MeanRunMetA[['FPR']][2,c(1,2,5)]-SD.OLS[c(1,2,5),'FPR'],#
    add=TRUE,errbar.col=colours[3])#
lines(MeanRunMetA[['FPR']][3,c(1,2,5)]~c(xAxes+.02)[c(1,2,3)],type='b',pch=4,lwd=3,lty=5,col=colours[3])    #
errbar(x=c(xAxes[c(1,2,3)]+.02),y=MeanRunMetA[['FPR']][3,c(1,2,5)],#
    yplus=MeanRunMetA[['FPR']][3,c(1,2,5)]+SD.MIX[c(1,2,5),'FPR'],#
    yminus=MeanRunMetA[['FPR']][3,c(1,2,5)]-SD.MIX[c(1,2,5),'FPR'],#
    add=TRUE,errbar.col=colours[3])#
    legend('topright',legend=c('Fixed Effect','OLS', 'Mixed Effect'),col=colours,#
        lwd=3,lty=c(1,3,5),bty='n',title='Pooling subjects:', cex=1.4)#
    axis(1,at=xAxes[c(1,2,3)],labels=metaLabels[c(1,2,5)],cex.axis=1.2)#
    axis(2,at=seq(0,.2,.05),cex.axis=1.2)
### List of mean overlap, power and FPR over all runs#
VarBS <- array(NA, dim=c(prod(DIM),numpoolmeths,NRUNS))#
### Load in the data#
for(i in 1:NRUNS){#
    # At run:#
    print(paste('At run ',i,sep=''))#
    # Working directory for this run#
    runWD <- paste(wd,'/Run_',i,'/MetaAnalyses/RanEffUn',sep='')#
#
    # Reading in the data#
    for(j in 1:numpoolmeths){#
        # Load in BVar#
        load(paste(runWD,'/',poolmeth[j],'/VarBS',sep=''))#
        # Put it in the vector#
        VarBS[,j,i] <- varianceBS#
        # Remove object#
        rm(varianceBS)  #
    }#
    # Remove runWD#
    rm(runWD)#
}
### List of mean overlap, power and FPR over all runs#
VarBS <- array(NA, dim=c(prod(DIM),numpoolmeths,NRUNS))#
### Load in the data#
for(i in 1:NRUNS){#
    # At run:#
    print(paste('At run ',i,sep=''))#
    # Working directory for this run#
    runWD <- paste(wd,'/Run_',i,'/MetaAnalyses/RanEffUn',sep='')#
#
    # Reading in the data#
    for(j in 1:numpoolmeths){#
        # Load in BVar#
        load(paste(runWD,'/',poolmeth[j],'/VarBS',sep=''))#
        # Put it in the vector#
        VarBS[,j,i] <- varianceBS#
        # Remove object#
        rm(varianceBS)  #
    }#
    # Remove runWD#
    rm(runWD)#
}
summary(VarBS)
options(digits=3)
for(i in 1:NRUNS){#
    for(j in 1:numpoolmeths){#
        #print(summary(VarBS[,j,i]))#
        print(paste('Min = ',min(VarBS[,j,i]),', mean = ', round(mean(VarBS[,j,i]),3), ', max = ', round(max(VarBS[,j,i]),3), sep=''))#
    }#
}
avgTau <- array(0,dim=c(numpoolmeths*NRUNS,numpoolmeths,NRUNS))#
avgTau <- c()#
for(i in 1:NRUNS){#
    for(j in 1:numpoolmeths){#
        avgTau <- c(avgTau,mean(VarBS[,j,i]))#
    }#
}
avgTauD <- data.frame(  'Tau' = avgTau,#
                        'Pooling' = rep(poolmeth,NRUNS),#
                        'Run' = rep(c(1:NRUNS),each=numpoolmeths))#
avgTauD[,'Run'] <- as.factor(avgTauD[,'Run'])#
avgTauD[,'Pooling'] <- factor(avgTauD[,'Pooling'],levels=poolmeth,labels=poolmeth)
# Create plot#
    colours <- brewer.pal(name="Blues", n=nlevels(avgTauD$Pooling)+3)#
colours <- c('#d0d1e6','#74a9cf','#0570b0')#
ggplot(avgTauD, aes(Pooling,Tau,fill=Pooling)) + #
geom_bar(stat='identity', position='dodge',widts=.5) +#
scale_fill_manual(values = colours,#
            labels=poolmeth) +#
            facet_wrap(~ Run,nrow=2,ncol=5) +#
    scale_x_discrete(name="") +#
    scale_y_continuous(name="Average Tau") +#
    ggtitle(paste("Average variance between studies for 10 runs.", sep=""))+#
        theme(plot.title = element_text(lineheight=.6, face="bold"))
mask <- readNIfTI('/Volumes/2_TB_WD_Elements_10B8_Han/PhD/IMAGENDATA/Data/10RunsSites/Run_6/GroupAnalysis/mask.nii',
verbose=FALSE, warn=-1, reorient=TRUE, call=NULL)[,,,1]
wd
mask <- readNIfTI('/Volumes/2_TB_WD_Elements_10B8_Han/PhD/IMAGENDATA/Data/10RunsSites/CognitiveTaskHPC/Run_6/GroupAnalysis/mask.nii',
verbose=FALSE, warn=-1, reorient=TRUE, call=NULL)[,,,1]
for(i in 1:3){#
    toPlot <- VarBS[,i,6]#
    toPlot <- array(toPlot,dim=DIM)#
    # Mask #
    idMask <- mask==0#
    maskedPlot <- toPlot#
    maskedPlot[idMask] <- NA#
    # Assign plots to i p variables#
    assign(paste('VarBSp',i,sep=''),#
        levelplot(maskedPlot,main=paste('VarBS with ' ,poolmeth[i],' pooling of subjects.',sep=''),#
        pretty=TRUE, col.regions = terrain.colors, xlab = 'X', ylab = 'Y', #
        xlim=c(0,DIM[1]),ylim=c(0,DIM[2]))#
    )#
}
groupR6 <- readNIfTI('/Volumes/2_TB_WD_Elements_10B8_Han/PhD/IMAGENDATA/Data/10RunsSites/CognitiveTaskHPC/Run_6/GroupAnalysis/thresh_zstat1.nii',
verbose=FALSE, warn=-1, reorient=TRUE, call=NULL)[,,]
# Mask#
groupR6[idMask] <- NA#
# Assign to levelplot#
assign('GroupRes',#
    levelplot(groupR6,main=paste('Group Analysis (GT) of Run 6: BH adjusted p-values < .001',sep=''),#
    pretty=TRUE, col.regions = terrain.colors, xlab = 'X', ylab = 'Y', #
    xlim=c(0,DIM[1]),ylim=c(0,DIM[2]))#
    )
grid.arrange(VarBSp1,VarBSp2,VarBSp3,GroupRes,ncol=2,nrow=2)
scenario=='FACES6'
if(scenario=='FACES6'){NRUNS <- 42;NSTEP <- 70}
RUNsequence <- rep(c(1:NRUNS),each=(NSTEP*2))
STEPsequence <- rep(rep(c(1:NSTEP),each=2),NRUNS)
GROUPsequence <- rep(c(1,2),(NSTEP*NRUNS))
scenario=
'FACES6'
if(scenario=='FACES6'){NRUNS <- 42;NSTEP <- 70}
RUNsequence <- rep(c(1:NRUNS),each=(NSTEP*2))
STEPsequence <- rep(rep(c(1:NSTEP),each=2),NRUNS)
GROUPsequence <- rep(c(1,2),(NSTEP*NRUNS))
index <- 114
run <- RUNsequence[index]
step <- STEPsequence[index]
group <- GROUPsequence[index]
run
step
group
suppressMessages(library(AnalyzeFMRI))
suppressMessages(library(lattice))
suppressMessages(library(oro.nifti))
# Print which libraries are needed
print("Need packages: AnalyzeFMRI, lattice and oro.nifti")
# Take arguments from master file
wd <-
"/Volumes/2_TB_WD_Elements_10B8_Han/PhD/IMAGENDATA/IMAGEN/IMAGEN/FACES/Renamed/TestAnalysis"
setwd(wd)
pmapDir <-
"/Volumes/2_TB_WD_Elements_10B8_Han/PhD/IMAGENDATA/IMAGEN/IMAGEN/FACES/Renamed/TestAnalysis"
pmap <- readNIfTI(paste(pmapDir,"/pval.nii.gz",sep=""), verbose=FALSE, warn=-1, reorient=TRUE, call=NULL)[,,]
pmapDir <-
"/Volumes/2_TB_WD_Elements_10B8_Han/PhD/IMAGENDATA/IMAGEN/IMAGEN/FACES/Renamed/TestAnalysis/stats"
pmap <- readNIfTI(paste(pmapDir,"/pval.nii.gz",sep=""), verbose=FALSE, warn=-1, reorient=TRUE, call=NULL)[,,]
mask <- readNIfTI(paste(wd,"/mask.nii",sep=""), verbose=FALSE, warn=-1, reorient=TRUE, call=NULL)[,,]
signLevel <- 0.05
DIM <- dim(pmap)
## Mask the p-values map
idMask <- mask==0
MPmap <- pmap
MPmap[idMask] <- NA
## Adjust P-values with Benjamin & Hochberg procedure (built in R function)
adjPvals <- array(p.adjust(pmap,method="BH"),dim=DIM)
## Threshold at signLevel BH p-value: significant P-values get 1!
idP <- adjPvals <= signLevel
threshPval <- adjPvals
threshPval[!idP] <- 0
threshPval[idP] <- 1
switchAxis <- threshPval[c(dim(threshPval)[1]:1),,]
pdf('GroupAnalysis.pdf')#
levelplot(threshPval,main=paste('Group Analysis: BH adjusted p-values < ', signLevel,sep=''),#
    pretty=TRUE, col.regions = terrain.colors, xlab = 'X', ylab = 'Y', #
    xlim=c(0,DIM[1]),ylim=c(0,DIM[2]))#
dev.off()
levelplot(threshPval,main=paste('Group Analysis: BH adjusted p-values < ', signLevel,sep=''),#
    pretty=TRUE, col.regions = terrain.colors, xlab = 'X', ylab = 'Y', #
    xlim=c(0,DIM[1]),ylim=c(0,DIM[2]))
MPmap
adjPvals
adjPvals1 <- array(p.adjust(pmap,method="BH"),dim=DIM)
adjPvals <- array(p.adjust(MPmap,method="BH"),dim=DIM)
adjPvals1-adjPvals
summary(adjPvals1-adjPvals)
hist(adjPvals1-adjPvals)
adjPvals <- array(p.adjust(MPmap,method="BH"),dim=DIM)
idP <- adjPvals <= signLevel
threshPval <- adjPvals
threshPval[!idP] <- 0
threshPval[idP] <- 1
threshPval
switchAxis <- threshPval[c(dim(threshPval)[1]:1),,]
levelplot(threshPval,main=paste('Group Analysis: BH adjusted p-values < ', signLevel,sep=''),#
    pretty=TRUE, col.regions = terrain.colors, xlab = 'X', ylab = 'Y', #
    xlim=c(0,DIM[1]),ylim=c(0,DIM[2]))
pdf('GroupAnalysis.pdf')#
levelplot(threshPval,main=paste('Group Analysis: BH adjusted p-values < ', signLevel,sep=''),#
    pretty=TRUE, col.regions = terrain.colors, xlab = 'X', ylab = 'Y', #
    xlim=c(0,DIM[1]),ylim=c(0,DIM[2]))#
dev.off()
laod('/Volumes/2_TB_WD_Elements_10B8_Han/PhD/IMAGENDATA/IMAGEN/IMAGEN/IMAGEN/OurIDs')
load('/Volumes/2_TB_WD_Elements_10B8_Han/PhD/IMAGENDATA/IMAGEN/IMAGEN/IMAGEN/OurIDs')
head(OurIDs)
1-pnorm(2.7)
1-pnorm(NA)
rm(list=ls())
# Set WD
wd <- "/Volumes/2_TB_WD_Elements_10B8_Han/PhD/IMAGENDATA/Data/FreddieFreeloader/Data/Faces/Scenario6/"
setwd(wd)
library(lattice)
library(gridExtra)
library(oro.nifti)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(Hmisc)
library(fslr)
source('~/Dropbox/PhD/PhDWork/Meta\ Analysis/R\ Code/Studie_FixRan/FixRanStudyGit.git/Development/functions.R')
# Print which libraries are needed
print("Need packages: oro.nifti, fslr, lattice, ggplot2, reshape2, RColorBrewer, gridExtra and the functions.R file")
# Dimension of the brains
DIM <- c(53,63,46)
# Number of runs (so far)
NRUNS <- 42
# Number of steps
NSTEP <- 70
# Load Center information
load('/Volumes/2_TB_WD_Elements_10B8_Han/PhD/IMAGENDATA/IMAGEN/IMAGEN/IMAGEN/OurIDsFace')
load('/Volumes/2_TB_WD_Elements_10B8_Han/PhD/IMAGENDATA/IMAGEN/IMAGEN/FACES/Renamed/OurIDsFace')
head(OurIDs)
# Matrix were data will get into
MatrixOverlap <- array(NA,dim=c(NSTEP,NRUNS))
# Vector where we will store the percentage of activated masked voxels
# As we have two images per step and run, we will take the average of the percentage.
PercAct <- array(NA,dim=c(NSTEP,NRUNS))
# Print statement
PriST <- (c(1:NRUNS)/NRUNS)[seq(1,NRUNS,length.out=10)][-10]
# New significance level
signLevel <- 0.05
IDprint <- c(i/NRUNS)==PriST
imageG1 <- try(readNIfTI(paste(wd,'/Run_',i,'/Step_',j,'/Group1','/thresh_zstat1.nii',sep=''))[,,],silent=TRUE)
maskG1 <- try(readNIfTI(paste(wd,'/Run_',i,'/Step_',j,'/Group1','/masked.nii.gz',sep=''))[,,],silent=TRUE)
i<-1
j<-1
imageG1 <- try(readNIfTI(paste(wd,'/Run_',i,'/Step_',j,'/Group1','/thresh_zstat1.nii',sep=''))[,,],silent=TRUE)
maskG1 <- try(readNIfTI(paste(wd,'/Run_',i,'/Step_',j,'/Group1','/masked.nii.gz',sep=''))[,,],silent=TRUE)
imageG2 <- try(readNIfTI(paste(wd,'/Run_',i,'/Step_',j,'/Group2','/thresh_zstat1.nii',sep=''))[,,],silent=TRUE)
imageG2
maskG2 <- try(readNIfTI(paste(wd,'/Run_',i,'/Step_',j,'/Group2','/masked.nii.gz',sep=''))[,,],silent=TRUE)
maskG2
idMaskG2 <- maskG2==0
imageG2[idMaskG2] <- NA
imageG2
sumMap <- imageG1+imageG2
# Minus map: know the voxels different in both images
minusMap <- imageG1-imageG2
minusMap
# For loop over all runs#
for(i in 1:NRUNS){#
    IDprint <- c(i/NRUNS)==PriST#
    if(any(IDprint)){#
        print(paste(round(PriST,2)[IDprint]*100, "% done"))#
    }#
    # For loop over all steps#
    for(j in 1:NSTEP){#
        # We have two images (G1 and G2) from group 1 and group 2: 0 is non significant and 1 is significant#
            # We also need to check whether both of the thresholded maps are present (if not, skip step)#
        #-------------------------------------------------------------------------------------------------------------##
        imageG1 <- try(readNIfTI(paste(wd,'/Run_',i,'/Step_',j,'/Group1','/thresh_zstat1.nii',sep=''))[,,],silent=TRUE)#
            if(!class(imageG1)=='array') next#
            maskG1 <- try(readNIfTI(paste(wd,'/Run_',i,'/Step_',j,'/Group1','/masked.nii.gz',sep=''))[,,],silent=TRUE)#
                if(!class(maskG1)=='array') next#
#
        #-------------------------------------------------------------------------------------------------------------##
        # Group 2   #
        imageG2 <- try(readNIfTI(paste(wd,'/Run_',i,'/Step_',j,'/Group2','/thresh_zstat1.nii',sep=''))[,,],silent=TRUE)#
            if(!class(imageG2)=='array') next#
            maskG2 <- try(readNIfTI(paste(wd,'/Run_',i,'/Step_',j,'/Group2','/masked.nii.gz',sep=''))[,,],silent=TRUE)#
                if(!class(maskG2)=='array') next#
#
        #-------------------------------------------------------------------------------------------------------------##
        # Summing image K and K-1 to know the voxels in both maps (1+1 = 2)#
        sumMap <- imageG1+imageG2#
        # Minus map: know the voxels different in both images#
        minusMap <- imageG1-imageG2#
            # Mask this image and the individual images#
        idMask <- maskG1*maskG2#
            minusMap[idMask] <- NA#
        idMaskG1 <- maskG1==0#
            imageG1[idMaskG1] <- NA#
        idMaskG2 <- maskG2==0#
            imageG2[idMaskG2] <- NA#
#
        #-------------------------------------------------------------------------------------------------------------# #
        # Union of activated voxels, voxels in image G1 and voxels in image G2#
        Vjt <- length(sumMap[which(sumMap==2)])#
        Vt <- length(imageG1[which(imageG1==1)])#
        Vj <- length(imageG2[which(imageG2==1)])#
        # Voxels different from both images#
        VjtS <- length(minusMap[which(minusMap==0)])#
#
        #-------------------------------------------------------------------------------------------------------------##
        # Put overlap in matrix#
        MatrixOverlap[j,i] <- round((Vjt)/(Vj + Vt - Vjt),6)#
#
        #-------------------------------------------------------------------------------------------------------------##
        # Now calculate percentage of activated masked voxels in imageG1 and imageG2#
        baseG1 <- sum(maskG1)#
        percG1 <- Vt/baseG1#
#
        baseG2 <- sum(maskG2)#
        percG2 <- Vt/baseG2#
        PercAct[j,i] <- round(mean(c(percG1,percG2)),4)#
#
        #-------------------------------------------------------------------------------------------------------------##
        # Remove objects#
        rm(Vjt,Vt,Vj,imageG1,imageG2,sumMap,idMask,idMaskG1,idMaskG2,baseG1,baseG2)#
    }#
if(i==NRUNS) print("100% done")#
}
MatrixOverlap
MatrixOverlap[is.nan(MatrixOverlap)] <- 0
complete.cases(t(MatrixOverlap))
## Prepare matrix#
Overlap.tmp <- matrix(MatrixOverlap,ncol=1)#
    sampleSize <- rep(seq(10,700,by=10),NRUNS)#
Overlap <- data.frame('overlap' = Overlap.tmp, 'size' = sampleSize)#
#
# Variables for plotting#
subjBreak <- c(seq(0,100,by=20),seq(100,700,by=50))#
#
##################
## ggplot: points#
##################
ggplot(Overlap, aes(x=factor(sampleSize),y=overlap)) + #
    geom_point() +#
    scale_x_discrete(breaks=subjBreak, name="Sample size")
# Data frame with overlap and percentage#
    perc <- matrix(PercAct,ncol=1)#
OverPerc <- data.frame('Value' = rbind(Overlap.tmp,perc),#
        'Size' = rep(sampleSize,2), #
        'Type' = c(rep('Overlap',length(perc)),rep('Percentage',length(perc))))#
    OverPerc$Type <- as.factor(OverPerc$Type)#
    OverPerc$Value <- as.numeric(OverPerc$Value)#
#
# Correlation#
corr <- round(cor(na.omit(perc),na.omit(Overlap.tmp)),2)#
#
ggplot(OverPerc, aes(x=factor(Size),y=Value)) + #
    geom_point(aes(colour=Type),position='identity',size=1.5) +#
    scale_x_discrete(breaks=subjBreak, name="Sample size") +#
    scale_y_continuous(name='Overlap') +#
    theme(plot.title = element_text(lineheight=.2, face="bold")) +#
    ggtitle('Overlap and percentage of masked voxels active.') +#
    annotate("text", y = .1, x = 60, label = paste('Correlation = ', corr,sep=''),size=5)
# how many observations are there?#
observations <- apply(MatrixOverlap,c(1),function(x){length(na.omit(x))})#
    # put in data frame#
    obs <- data.frame('count' = observations, 'size' = seq(10,700,by=10))#
#
ggplot(obs, aes(x=factor(size), y=count))+#
    geom_bar(stat='identity', colour='#1f78b4',fill='#a6cee3') +#
    scale_x_discrete(breaks=seq(0,700,by=50), name="sample size") +#
    ggtitle('Incomplete data: amount of comparissons in each sample size.') +#
            theme(plot.title = element_text(lineheight=.2, face="bold")) + #
            scale_y_continuous(name='Amount of data points')+#
            theme_bw()
# Calculate averages#
tmp <- aggregate(OverPerc,by=list(OverPerc$Size,OverPerc$Type),mean,na.rm=TRUE)#
AvgOverPerc <- tmp[,c(2:4)]#
    names(AvgOverPerc) <- c('Type', 'Value', 'Size')#
#
ggplot(AvgOverPerc, aes(x=factor(Size),y=Value,group=Type)) + #
    geom_line(aes(colour=Type)) +#
    geom_point(aes(colour=Type),position='identity',size=1.5) +#
    scale_x_discrete(breaks=subjBreak, name="Sample size") +#
    scale_y_continuous(name='Overlap') +#
    theme(plot.title = element_text(lineheight=.2, face="bold")) +#
    ggtitle('Average overlap and percentage of masked voxels active.')+#
    annotate("text", y = .1, x = 60, label = paste('Correlation = ', corr,sep=''),size=5)
readNIfTI('~/Desktop/labels_Neuromorphometrics.nii')[,,]
map <- readNIfTI('~/Desktop/labels_Neuromorphometrics.nii')[,,]
map
summary(map)
levelplot(map)
rm(list=ls())
gc(verbose = FALSE)
wd <- "/Volumes/2_TB_WD_Elements_10B8_Han/PhD/IMAGENDATA/Data/FreddieFreeloader/Script.git/Analyses/Faces/Scenario9"
setwd(wd)
library(lattice)
library(gridExtra)
library(oro.nifti)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(Hmisc)
library(fslr)
source('~/Dropbox/PhD/PhDWork/Meta\ Analysis/R\ Code/Studie_FixRan/FixRanStudyGit.git/Development/functions.R')
# Print which libraries are needed
print("Need packages: oro.nifti, fslr, lattice, ggplot2, reshape2, RColorBrewer, gridExtra and the functions.R file")
# Location of the data
dat <- '/Volumes/2_TB_WD_Elements_10B8_Han/PhD/IMAGENDATA/Data/FreddieFreeloader/Data/Faces/Scenario6'
# Dimension of the brains
DIM <- c(53,63,46)
# Number of runs (so far)
NRUNS <- 42
# Number of steps
NSTEP <- 70
set.seed(11121990)
load('/Volumes/2_TB_WD_Elements_10B8_Han/PhD/IMAGENDATA/IMAGEN/IMAGEN/FACES/Renamed/OurIDsFace')
head(OurIDsFace)
# Baseline, starting significance level
signLevel <- 0.05
# Adaptive Thresholding#
AdapThresholding <- function(pvals,DIM,signLevel,idMask){#
    # No adjustment for multiple testing with P-values with Benjamin & Hochberg procedure (built in R function)#
    adjPvals <- pvals#
#
    # Threshold at signLevel BH p-value: significant P-values get 1!#
    idP <- adjPvals <= signLevel#
    threshPval <- adjPvals#
    threshPval[!idP] <- 0#
    threshPval[idP] <- 1#
#
    # Calculate starting (base) percentage of activated masked voxels#
    maskedVox <- sum(!idMask)       # idMask: TRUE for voxel outside of mask. Hence sum of reverse. #
    Vt <- sum(idP,na.rm=TRUE)#
    percentage <- Vt/maskedVox#
#
    # Now if percentage is below .19, increase threshold (by .001), if above .21; decrease by .01#
    while(percentage < 0.19 | percentage > 0.21){#
        if(percentage < 0.19){#
            if(signLevel >= 0.995){#
                signLevel <- signLevel + 0.00001#
            }else{#
                signLevel <- signLevel + runif(1,min=.0001,max=0.005)#
            }#
        }#
        if(percentage > 0.21){#
            if(signLevel <= 0.005){#
                signLevel <- signLevel - 0.00001    #
            }else{#
                signLevel <- signLevel - runif(1,min=.0001,max=0.005)#
            }#
        }#
        # Recalculate thresholded map and percentage#
        idP <- adjPvals <= signLevel#
        threshPval <- adjPvals#
        threshPval[!idP] <- 0#
        threshPval[idP] <- 1#
        Vt <- sum(idP,na.rm=TRUE)#
        percentage <- Vt/maskedVox#
        }#
#
    # Return percentage, signLevel and threshPval#
    result <- list('signLevel' = signLevel,'percentage' = percentage,'SPM' = threshPval)#
    return(result)#
}
MatrixOverlapAdap <- array(NA,dim=c(NSTEP,NRUNS))
# Vector where we will store the final percentage of activated masked voxels
# As we have two images per step and run, we will take the average of the percentage.
PercActAdap <- array(NA,dim=c(NSTEP,NRUNS))
# Vector to store average of percentage activated masked voxels in each step and run
SignLevels <- array(NA,dim=c(NSTEP,NRUNS))
# Vector for activation map of first run only
SPMRun1G1 <- array(NA,dim=c(prod(DIM),NSTEP))
SPMRun1G2 <- array(NA,dim=c(prod(DIM),NSTEP))
i<-j<-1
print(paste('Step ',j,sep=''))
imageG1 <- try(readNIfTI(paste(dat,'/Run_',i,'/Step_',j,'/Group1','/stats/zstat1.nii',sep=''))[,,],silent=TRUE)
imageG1
maskG1 <- try(readNIfTI(paste(dat,'/Run_',i,'/Step_',j,'/Group1','/masked.nii.gz',sep=''))[,,],silent=TRUE)
maskG1
idMaskG1 <- maskG1==0
imageG1[idMaskG1] <- NA
imageG2 <- try(readNIfTI(paste(dat,'/Run_',i,'/Step_',j,'/Group2','/stats/zstat1.nii',sep=''))[,,],silent=TRUE)
imageG2
maskG2 <- try(readNIfTI(paste(dat,'/Run_',i,'/Step_',j,'/Group2','/masked.nii.gz',sep=''))[,,],silent=TRUE)
maskG2
idMaskG2 <- maskG2==0
imageG2[idMaskG2] <- NA
pvalsG1 <- 1-pnorm(imageG1)
pvalsG2 <- 1-pnorm(imageG2)
# Function AdapThresholding: calculates adjusted p-values, then finds correct significance level according to percentage.
adapG1 <- AdapThresholding(pvalsG1,DIM,signLevel,idMaskG1)
signLevel <- adapG1$signLevel
perc <- adapG1$percentage
# Image G2
adapG2 <- AdapThresholding(pvalsG2,DIM,signLevel,idMaskG2)
signLevel <- round(mean(c(signLevel,adapG2$signLevel)),6)
perc <- round(mean(c(perc,adapG2$percentage)),4)
sumMap <- adapG1$SPM+adapG2$SPM
# Minus map: know the voxels different in both images
minusMap <- adapG1$SPM-adapG2$SPM
# Mask this image
idMask <- maskG1*maskG2
minusMap[idMask] <- NA
# Union of activated voxels, voxels in image G1 and voxels in image G2
Vjt <- length(sumMap[which(sumMap==2)])
Vt <- length(adapG1$SPM[which(adapG1$SPM==1)])
Vj <- length(adapG2$SPM[which(adapG2$SPM==1)])
# Voxels different from both images
VjtS <- length(minusMap[which(minusMap==0)])
# Put overlap, percentage and signLevel in matrix
MatrixOverlapAdap[j,i] <- round((Vjt)/(Vj + Vt - Vjt),6)
PercActAdap[j,i] <- perc
SignLevels[j,i] <- signLevel
# Matrix were overlap results will get into#
MatrixOverlapAdap <- array(NA,dim=c(NSTEP,NRUNS))#
#
# Vector where we will store the final percentage of activated masked voxels#
    # As we have two images per step and run, we will take the average of the percentage. #
PercActAdap <- array(NA,dim=c(NSTEP,NRUNS))#
#
# Vector to store average of percentage activated masked voxels in each step and run#
SignLevels <- array(NA,dim=c(NSTEP,NRUNS))#
#
# Vector for activation map of first run only#
SPMRun1G1 <- array(NA,dim=c(prod(DIM),NSTEP))#
SPMRun1G2 <- array(NA,dim=c(prod(DIM),NSTEP))#
# For loop over all runs#
for(i in 1:NRUNS){#
    print(paste('--------------- Run ', i,'--------------',sep=''))#
    # For loop over all steps#
    for(j in 1:NSTEP){#
        print(paste('Step ',j,sep=''))#
        ####################################################################
        # We have two images (G1 and G2) from group 1 and group 2: 0 is non significant and 1 is significant#
            # We also need to check whether both of the thresholded maps are present (if not, skip step)#
        imageG1 <- try(readNIfTI(paste(dat,'/Run_',i,'/Step_',j,'/Group1','/stats/zstat1.nii',sep=''))[,,],silent=TRUE)#
            if(!class(imageG1)=='array') next#
            maskG1 <- try(readNIfTI(paste(dat,'/Run_',i,'/Step_',j,'/Group1','/masked.nii.gz',sep=''))[,,],silent=TRUE)#
                if(!class(maskG1)=='array') next#
            idMaskG1 <- maskG1==0#
            imageG1[idMaskG1] <- NA#
            # Check second image (there is no point in calculating percentages and stuff if not both images are available)#
        imageG2 <- try(readNIfTI(paste(dat,'/Run_',i,'/Step_',j,'/Group2','/stats/zstat1.nii',sep=''))[,,],silent=TRUE)#
            if(!class(imageG2)=='array') next#
            maskG2 <- try(readNIfTI(paste(dat,'/Run_',i,'/Step_',j,'/Group2','/masked.nii.gz',sep=''))[,,],silent=TRUE)#
                if(!class(maskG2)=='array') next#
            idMaskG2 <- maskG2==0#
            imageG2[idMaskG2] <- NA#
#
        ####################################################################
        # Now that we have z-statistic images with its masks, calculate BF-adjusted p-values using baseline significance level#
        pvalsG1 <- 1-pnorm(imageG1)#
        pvalsG2 <- 1-pnorm(imageG2)#
        # Function AdapThresholding: calculates adjusted p-values, then finds correct significance level according to percentage.#
        adapG1 <- AdapThresholding(pvalsG1,DIM,signLevel,idMaskG1)#
            signLevel <- adapG1$signLevel#
            perc <- adapG1$percentage#
#
        # Image G2#
        adapG2 <- AdapThresholding(pvalsG2,DIM,signLevel,idMaskG2)#
            signLevel <- round(mean(c(signLevel,adapG2$signLevel)),6)#
            perc <- round(mean(c(perc,adapG2$percentage)),4)#
        ####################################################################
        # Calculate overlap: summing image K and K-1 to know the voxels in both maps (1+1 = 2)#
        sumMap <- adapG1$SPM+adapG2$SPM#
        # Minus map: know the voxels different in both images#
        minusMap <- adapG1$SPM-adapG2$SPM#
            # Mask this image#
        idMask <- maskG1*maskG2#
            minusMap[idMask] <- NA#
        # Union of activated voxels, voxels in image G1 and voxels in image G2#
        Vjt <- length(sumMap[which(sumMap==2)])#
        Vt <- length(adapG1$SPM[which(adapG1$SPM==1)])#
        Vj <- length(adapG2$SPM[which(adapG2$SPM==1)])#
        # Voxels different from both images#
        VjtS <- length(minusMap[which(minusMap==0)])#
        # Put overlap, percentage and signLevel in matrix#
        MatrixOverlapAdap[j,i] <- round((Vjt)/(Vj + Vt - Vjt),6)#
        PercActAdap[j,i] <- perc#
        SignLevels[j,i] <- signLevel#
        ####################################################################
        # From run 1: take the thresholded maps for levelplotting#
        if(i==1){#
            assign(paste('ThrMap1_',j,sep=''),#
            levelplot(adapG1$SPM,main=paste('Group 1 analysis of step ' ,j,sep=''),#
            pretty=TRUE, col.regions = terrain.colors, xlab = 'X', ylab = 'Y', #
            xlim=c(0,DIM[1]),ylim=c(0,DIM[2]))#
            )#
            assign(paste('ThrMap2_',j,sep=''),#
            levelplot(adapG2$SPM,main=paste('Group 2 analysis of step ' ,j,sep=''),#
            pretty=TRUE, col.regions = terrain.colors, xlab = 'X', ylab = 'Y', #
            xlim=c(0,DIM[1]),ylim=c(0,DIM[2]))#
            )#
            # Activation maps#
            SPMRun1G1[,j] <- adapG1$SPM#
            SPMRun1G2[,j] <- adapG2$SPM#
        }#
        # Remove objects#
        rm(Vjt,Vt,Vj,imageG1,imageG2,sumMap,maskG1,idMaskG1,maskG2,idMaskG2,adapG1,adapG2)#
        gc()#
    }#
if(i==NRUNS) print("100% done")#
}
paste(wd,'/MatrixOverlapAdap',sep='')
save(MatrixOverlapAdap,file=paste(wd,'/MatrixOverlapAdapFace',sep=''))
save(PercActAdap,file=paste(wd,'/PercActSplitAdapFace',sep=''))
save(SignLevels,file=paste(wd,'/SignLevelsAdapFace',sep=''))
